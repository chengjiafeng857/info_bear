# Cantonese LLM Fine-Tuning: Qwen2.5-3B-Instruct

## Overview

This repository contains the code for fine-tuning the `Qwen/Qwen2.5-3B-Instruct` language model for improved Cantonese language instruction-following capabilities. The project utilizes the `hon9kon9ize/yue-alpaca` dataset and leverages the Unsloth library for efficient LoRA-based fine-tuning.

## Features

* **Cantonese Focus:** Fine-tuned specifically for understanding and generating responses based on Cantonese language instructions.
* **Efficient Fine-Tuning:** Uses Unsloth and LoRA with 4-bit quantization for faster training and reduced memory usage, making it suitable for platforms like Google Colab.
* **Evaluation Included:** Provides a notebook (`Eval.ipynb`) for comparing the fine-tuned model against the original base model using human evaluation.

## Model Details

* **Base Model:** [Qwen/Qwen2.5-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct)
* **Fine-tuned Model Adapters:** [jackf857/canton_fine-tuned_qwen3b-instruct](https://huggingface.co/jackf857/canton_fine-tuned_qwen3b-instruct)
* **Dataset:** [hon9kon9ize/yue-alpaca](https://huggingface.co/datasets/hon9kon9ize/yue-alpaca)

## Repository Contents

* `Qwen2_5_(3B)_Alpaca_cantonese_fine-tuned.ipynb`: Jupyter notebook detailing the data preparation, LoRA setup, fine-tuning process using `SFTTrainer`, and saving the model adapters.
* `Eval.ipynb`: Jupyter notebook for loading the base and fine-tuned models and conducting a comparative human evaluation.
* (Potentially) `human_evaluation.csv`: CSV file generated by `Eval.ipynb` to store human evaluation scores.

## Setup

1.  **Clone the repository:**
    ```bash
    git clone <repository-url>
    cd <repository-name>
    ```
2.  **Install dependencies:** The notebooks include `pip install` commands for necessary libraries. Ensure you have a compatible environment (Python, CUDA). Key libraries include `unsloth`, `torch`, `transformers`, `trl`, `datasets`, `accelerate`, `bitsandbytes`, `peft`, `pandas`, `ipywidgets`. A requirements file could be generated for easier setup.
    ```bash
    # Example for Colab (see notebooks for specific commands)
    pip install "unsloth[colab-new] @ git+[https://github.com/unslothai/unsloth.git](https://github.com/unslothai/unsloth.git)"
    pip install --no-deps xformers==0.0.29.post3 trl==0.15.2 peft accelerate bitsandbytes
    pip install sentencepiece protobuf datasets huggingface_hub hf_transfer pandas ipywidgets
    ```

## Usage

1.  **Fine-Tuning:** Open and run the cells in `Qwen2_5_(3B)_Alpaca_cantonese_fine-tuned.ipynb`. This will:
    * Load the base model and tokenizer.
    * Load and format the dataset.
    * Configure and run the `SFTTrainer`.
    * Allow saving the trained LoRA adapters locally or pushing them to the Hugging Face Hub.
2.  **Evaluation:** Open and run the cells in `Eval.ipynb`. This will:
    * Load the original base model and the fine-tuned LoRA adapters.
    * Generate responses from both models for sample prompts.
    * Provide an interface for human scoring and calculate the average score.

## Notes

* Ensure you have access to suitable hardware (GPU with sufficient VRAM, especially for fine-tuning).
* You might need a Hugging Face token with write access to push the model to the Hub.
